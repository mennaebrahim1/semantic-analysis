{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "e_dl0cFzSb3a",
        "U2YcPbDmTC_8",
        "vKpDe64BTdV_",
        "FJ6o-5H5cOG4",
        "tYhrm2-3YZ_j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n"
      ],
      "metadata": {
        "id": "bbPfbcHKSPSU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7RnK7_ImkSj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d012a7b4-35bc-4c3b-8864-1c245b29d695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2023.11.17)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15715 sha256=bc5609589425609a0c512706d64b1f8f73379c5cff65e7dffdb3ca79062599d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=504d3f9203b8c3f61ef189746bc150cd839fed15921814889ce68c6108645eeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "32036\n",
            "1000\n",
            "0\n",
            "review_description    False\n",
            "rating                False\n",
            "dtype: bool\n",
            "0\n",
            "ID                    False\n",
            "review_description    False\n",
            "dtype: bool\n",
            "                                  review_description  rating\n",
            "0  شركه زباله و سواقين بتبرشم و مفيش حتي رقم للشك...      -1\n",
            "1  خدمة الدفع عن طريق الكي نت توقفت عندي اصبح فقط...       1\n",
            "2  تطبيق غبي و جاري حذفه ، عاملين اكواد خصم و لما...      -1\n",
            "3  فعلا تطبيق ممتاز بس لو فى امكانية يتيح لمستخدم...       1\n",
            "4  سيء جدا ، اسعار رسوم التوصيل لا تمت للواقع ب ص...      -1\n",
            "   ID                                 review_description\n",
            "0   1  اهنئكم على خدمه العملاء في المحادثه المباشره م...\n",
            "1   2  ممتاز جدا ولكن اتمنى ان تكون هناك بعض المسابقا...\n",
            "2   3    كل محملته يقول تم ايقاف حطيت2 عشان تسوون الخطاء\n",
            "3   4                                            شغل طيب\n",
            "4   5                                         بعد ماجربت\n",
            "                 ***************** فارع        ****************\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "#load data done\n",
        "import os\n",
        "\n",
        "!pip install googletrans\n",
        "!pip install langdetect\n",
        "\n",
        "from googletrans import Translator\n",
        "from langdetect import detect\n",
        "\n",
        "#display data\n",
        "column_names = [\"review_description\", \"rating\"]\n",
        "train_data = pd.read_excel(\"/content/train.xlsx\", names=column_names)\n",
        "test_data = pd.read_csv(\"/content/test _no_label.csv\",encoding=\"utf-8\",dtype={'review_description': str})\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "\n",
        "train_data[train_data.isnull().any(axis=1)].head()\n",
        "test_data[train_data.isnull().any(axis=1)].head()\n",
        "print(np.sum(train_data.isnull().any(axis=1)))   #done\n",
        "print(train_data.isnull().any(axis=0))\n",
        "\n",
        "print(np.sum(test_data.isnull().any(axis=1)))   #done\n",
        "print(test_data.isnull().any(axis=0))\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "#display(train_data)\n",
        "#display(test_data)\n",
        "\n",
        "#print(train_data.head())\n",
        "#print(test_data.head())\n",
        "#print(\"***************************************************************************\")\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].astype(str)\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].astype(str)\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "#######################################  NOTE  #############################################################\n",
        "test_data[\"review_description\"]  =test_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "\n",
        "for letter in '#.][!XR':\n",
        "    train_data[\"review_description\"] = train_data[\"review_description\"].astype(str).str.replace(letter,'')\n",
        "    test_data[\"review_description\"] = test_data[\"review_description\"].astype(str).str.replace(letter,'')\n",
        "\n",
        "train_data.replace('', np.nan, inplace=True)\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"                 ***************** فارع        ****************\")\n",
        "print(train_data[\"review_description\"].isnull().sum())\n",
        "#train_data.dropna(inplace=True)\n",
        "#print(len(train_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing"
      ],
      "metadata": {
        "id": "e_dl0cFzSb3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "#from translate import Translator\n",
        "from googletrans import Translator\n",
        "import nltk\n",
        "from googletrans import Translator\n",
        "#from translate import Translator\n",
        "#done but with limimted trials\n",
        "def translate_EtoA(english_text):\n",
        "    # Create an instance of the Translator\n",
        "    translator = Translator(service_urls=['translate.google.com'],to_lang=\"ar\")\n",
        "    # Translate English text to Arabic\n",
        "    translation = translator.translate(english_text)\n",
        "    # Access the translated text\n",
        "    arabic_text = translation\n",
        "    return arabic_text\n",
        "\n",
        "#!pip install emojis\n",
        "#!pip install emoji\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import csv\n",
        "!pip install emojis\n",
        "!pip install emoji\n",
        "import emojis\n",
        "import emoji\n",
        "nltk.download('stopwords')\n",
        "\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('arabic')) - {'نعم','لا', 'ليس', 'ليست', 'مش', 'ما','غير','أقبل','ليس','ليسا','ليست','ليستا','ليسوا','لست','لستم','لستما','لستن','لسن','لسنا','واو'}\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return \" \".join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "def build_emoji_dictionary(csv_file):\n",
        "    emoji_dict = {}\n",
        "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            emoji = row[0]\n",
        "            text = row[1]\n",
        "            emoji_dict[emoji] = text\n",
        "    return emoji_dict\n",
        "\n",
        "def replace_emojis(emoji_dict, sentence):\n",
        "    emojis = emoji.emoji_list(sentence)\n",
        "    for emo in emojis:\n",
        "        if emo['emoji'] in emoji_dict:\n",
        "            # Replace the emoji with the corresponding text surrounded by asterisks\n",
        "            sentence = sentence.replace(emo['emoji'], '*' + emoji_dict[emo['emoji']] + '* ')\n",
        "    return sentence\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def decode_emojis(text):\n",
        "    # Replace emojis with Arabic words\n",
        "    decoded_text = emojis.decode(text)\n",
        "    return decoded_text\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "emoji_dict = build_emoji_dictionary(\"/content/emojis.csv\") #Assuming the CSV file is in the same directory\n",
        "# in Arabic syllabels are represented by small signs above or below\n",
        "# each character, we will remove them\n",
        "# and see if they affect our prediction model\n",
        "!pip install pyarabic\n",
        "import pyarabic.araby as araby\n",
        "\n",
        "def remove_diactrics(text):\n",
        "    return araby.strip_diacritics(text)\n",
        "\n",
        "def processPost(text,emoji_dict):\n",
        "    #Replace @username with empty string\n",
        "    text = re.sub(r\"\\s+\", \" \", str(text).strip())\n",
        "\n",
        "    text = re.sub('@[^\\s]+', ' ', text)\n",
        "    #Convert www.* or https?://* to \" \"\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
        "    #remove nums\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    #Replace #word with word\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "    # remove punctuations\n",
        "    text= remove_punctuations(text)\n",
        "    text = normalize_arabic(text)\n",
        "    #handle emojes\n",
        "    text = replace_emojis(emoji_dict, text)\n",
        "\n",
        "    text= remove_punctuations(text)\n",
        "    # Translate English to Arabic\n",
        "    #text = translate_EtoA(text)  done but with limimted trials\n",
        "\n",
        "    # remove repeated letters\n",
        "    text = remove_repeating_char(text)\n",
        "    text = remove_stop_words(text)\n",
        "    #print(text)\n",
        "    text = remove_diactrics(text)\n",
        "    return text\n",
        "\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"              *****************    فارع        ****************\")\n",
        "print(train_data[\"review_description\"].isnull().sum())\n",
        "#print(train_data.shape[0])\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(lambda x: processPost(x,emoji_dict))\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].apply(lambda x: processPost(x,emoji_dict))\n",
        "\n",
        "\n",
        "train_data.replace('', np.nan, inplace=True)\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"                 ***************** فارع        ****************\")\n",
        "print(train_data[\"review_description\"].isnull().sum())\n",
        "#train_data.dropna(inplace=True)\n",
        "#print(len(train_data))\n",
        "\n",
        "test_data.replace('', np.nan, inplace=True)\n",
        "test_data[\"review_description\"] = test_data[\"review_description\"].fillna({\"review_description\":\"مجهول\"})\n",
        "print(\"                 ***************** فارع        ****************\")\n",
        "print(test_data[\"review_description\"].isnull().sum())\n",
        "#test_data.dropna(inplace=True)\n",
        "#print(len(test_data))\n",
        "#6 nana cases to be asked 3 number cases  ,1 case :'نناا'  , cases 'ععععع'\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(str)\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(tokenizer.tokenize)\n",
        "\n",
        "\n",
        "#stemming   to try\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "!pip install nltk\n",
        "stemmer = ISRIStemmer()\n",
        "\n",
        "def stem(text):\n",
        "    stemmed = []\n",
        "    for word in text:\n",
        "        stemmed.append(stemmer.stem(word))\n",
        "    return stemmed\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "def engstem(text):\n",
        "    e_stemmer = SnowballStemmer(\"english\")\n",
        "    stemmed = []\n",
        "    for word in text:\n",
        "        stemmed.append(e_stemmer.stem(word))\n",
        "    return stemmed\n",
        "\n",
        "\n",
        "#train_data[\"review_description\"] = train_data[\"review_description\"].apply(stem)\n",
        "#train_data[\"review_description\"] = train_data[\"review_description\"].apply(engstem)\n",
        "\n",
        "# Join the stemmed tokens back into a string\n",
        "train_data[\"review_description\"] = train_data[\"review_description\"].apply(lambda x: ' '.join(x))\n",
        "\n",
        "train_data[\"review_description\"]"
      ],
      "metadata": {
        "id": "oQTyEkTPnhO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f3eab1-4097-404a-c618-23970e191639"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emojis\n",
            "  Downloading emojis-0.7.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: emojis\n",
            "Successfully installed emojis-0.7.0\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n",
            "              *****************    فارع        ****************\n",
            "3\n",
            "                 ***************** فارع        ****************\n",
            "247\n",
            "                 ***************** فارع        ****************\n",
            "6\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        شركه زباله سواقين بتبرشم مفيش حتي رقم لشكاوي ا...\n",
              "1        خدمه الدفع طريق الكي نت توقفت عندي اصبح فقط ال...\n",
              "2        تطبيق غبي جاري حذفه عاملين اكواد خصم نستخدمها ...\n",
              "3        فعلا تطبيق متاز امكانيه يتيح لمستخدم التطبيق ا...\n",
              "4              سيء جدا اسعار رسوم التوصيل لا تمت لواقع صله\n",
              "                               ...                        \n",
              "32031    التطبيق اصبح سيء لغايه نقوم بطلب لا يتم وصول ا...\n",
              "32032                                           y love you\n",
              "32033                 الباقه بتخلص وبشحن مرتين باقه اضافيه\n",
              "32034    تطبيق فاشل وصلني الطلب ناقص ومش ينفع اعمل حاجه...\n",
              "32035                                      ليش ما يفتح معي\n",
              "Name: review_description, Length: 32036, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install tensorflow_version"
      ],
      "metadata": {
        "id": "U2YcPbDmTC_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "try:\n",
        "    %tensorflow_version 2.X\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "#!pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "xlr9b4Sjkqr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3cde0d-9828-43cb-ed26-704d8bea5523"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and Embedding"
      ],
      "metadata": {
        "id": "vKpDe64BTdV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "#regulatization\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "X_train = train_data['review_description']\n",
        "y_train = train_data['rating']\n",
        "x_test = test_data['review_description'].astype(str).tolist()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_tokenized = tokenizer.texts_to_sequences(X_train)\n",
        "X_tokenized_test =tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Padding sequences\n",
        "max_len = max([len(x) for x in X_tokenized])\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_len, padding='post')\n",
        "X_test_padded = pad_sequences(X_tokenized_test, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "# Encoding labels\n",
        "y_train_encoded = y_train + 1  # Assuming 3 classes\n",
        "y_train_encoded = to_categorical(y_train, num_classes=3)\n",
        "#print(y_train_encoded)\n",
        "#print(y_train_encoded.argmax(axis=-1))"
      ],
      "metadata": {
        "id": "ujd_F5MXpVav"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict the sentiments"
      ],
      "metadata": {
        "id": "FJ6o-5H5cOG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def Predict_sentiments(model,test):\n",
        "  predictions = model.predict(test)\n",
        "  print(predictions)\n",
        "  # Convert probabilities to class labels\n",
        "  predicted_classes = predictions.argmax(axis=-1)\n",
        "  predicted_classes = np.vectorize({2: -1, 1: 1, 0: 0}.get)(predicted_classes)  # To shift (0, 1, 2) to (-1, 0, 1)\n",
        "  print('**********************************************************************************')\n",
        "\n",
        "  print(predicted_classes)\n",
        "\n",
        "  predicted_labels = [np.argmax(pred) for pred in predicted_classes]\n",
        "  # Create a DataFrame with the original labels and predicted labels# Add the predicted labels as a new column to the existing DataFrame\n",
        "  test_data['Predicted_Label'] = predicted_labels\n",
        "  # Save back to the CSV file\n",
        "  test_data.to_csv('/content/test _no_label.csv', index=False)"
      ],
      "metadata": {
        "id": "WhGPPe8YcK1m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "c-_Vev06UBo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# !pip install tensorflow_addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, ff_dim, num_transformer_blocks, output_dim, rate=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.attention = tfa.layers.MultiHeadAttention(head_size=num_heads, num_heads=num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(d_model),\n",
        "        ])\n",
        "        #self.gru_layer = layers.GRU(16, return_sequences=True)    # Added GRU  layer\n",
        "        #self.lstm_layer = layers.LSTM(16, return_sequences=True)  # Added LSTM layer\n",
        "\n",
        "        self.rnn_layer = layers.GRU(16, return_sequences=True)    # Added GRU  layer\n",
        "\n",
        "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "        self.dense = layers.Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "      x = self.embedding(inputs)\n",
        "\n",
        "      # Reshape x to (batch_size, sequence_length, num_features)\n",
        "      x = tf.reshape(x, [-1, tf.shape(x)[1], self.embedding.output_dim])\n",
        "\n",
        "      attention_output = self.attention([x, x, x])\n",
        "      x = x + attention_output\n",
        "      x = self.rnn_layer(x)\n",
        "      #x = self.lstm_layer(x)  # Added LSTM layer\n",
        "      x = self.layer_norm1(x)\n",
        "      x = self.ffn(x)\n",
        "      x = self.dropout1(x)\n",
        "      x = x + attention_output\n",
        "      x = self.layer_norm2(x)\n",
        "      x = self.pooling(x)\n",
        "      return self.dense(x)\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "!pip install keras_tuner\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_transformer_model(hp):\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    num_transformer_blocks = 2\n",
        "    output_dim = 3  # Adjust based on your dataset\n",
        "\n",
        "    d_model = hp.Int('d_model', min_value=32, max_value=128, step=32)\n",
        "    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)\n",
        "    ff_dim = hp.Int('ff_dim', min_value=32, max_value=128, step=32)\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
        "\n",
        "    model = TransformerModel(vocab_size, d_model, num_heads, ff_dim, num_transformer_blocks, output_dim, rate=dropout_rate)\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_transformer_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='transformer_tuning'\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(\n",
        "    X_padded, y_train_encoded,\n",
        "    epochs=10,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''# Set hyperparameters\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "d_model = 64  # embedding dimension\n",
        "num_heads = 4\n",
        "ff_dim = 64  # feedforward dimension\n",
        "num_transformer_blocks = 2\n",
        "output_dim = 3  # Number of classes\n",
        "\n",
        "# Create and compile the model\n",
        "model = TransformerModel(vocab_size, d_model, num_heads, ff_dim, num_transformer_blocks, output_dim)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (-epoch / 20))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "# Train the model\n",
        "model.fit(X_padded, y_train_encoded, epochs=5, batch_size=16,validation_split=0.2,callbacks=[early_stopping, lr_scheduler])\n",
        "\n",
        "#Predict_sentiments(model,X_test_padded)\n",
        "#print(test_data['Predicted_Label'])'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "WLipq7JhsjZK",
        "outputId": "677d199f-6ecf-4cbc-e626-8bf764122af3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 08m 16s]\n",
            "val_accuracy: 0.8416042327880859\n",
            "\n",
            "Best val_accuracy So Far: 0.8416042327880859\n",
            "Total elapsed time: 01h 05m 58s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-289df61cf20e>\u001b[0m in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mbest_hyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best hyperparameters:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_hyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaseTuner.get_best_hyperparameters() got an unexpected keyword argument 'num_models'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "print('Best hyperparameters:', best_hyperparameters.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW20LtQ30TTm",
        "outputId": "09e59118-faa6-4209-98c1-84fe01031998"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'d_model': 96, 'num_heads': 8, 'ff_dim': 128, 'dropout_rate': 0.1, 'learning_rate': 0.00022491452958964034}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = best_model.predict(X_test_padded)\n",
        "# Convert probabilities to class labels\n",
        "predicted_classes = predictions.argmax(axis=-1)\n",
        "predicted_classes = np.vectorize({2: -1, 1: 1, 0: 0}.get)(predicted_classes)  # To shift (0, 1, 2) to (-1, 0, 1)\n",
        "#predicted_labels = [np.argmax(pred) for pred in predicted_classes]\n",
        "print(predicted_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O-FMiW_mYPn",
        "outputId": "40d8b64a-5d53-482c-bef5-5ed601bd707b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 2s 23ms/step\n",
            "[ 1  1  1  1  1  1 -1 -1  1 -1  1  1  1 -1 -1  1 -1 -1  1 -1  1  1  1  1\n",
            " -1 -1  1  1  1 -1  1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1 -1  1 -1\n",
            "  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1  1  1 -1  1 -1 -1 -1  1\n",
            "  1 -1 -1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
            " -1 -1 -1  1  1 -1  1 -1  1  1 -1 -1  1  1  1  1 -1 -1  1 -1  1  1  1  1\n",
            "  1  1  1  1  1  1  1 -1  1  1 -1 -1  1  1  1 -1 -1  1  1  1  1 -1  1 -1\n",
            "  1 -1 -1 -1  1  1  1  1  1 -1 -1  1  1 -1  1  1  1  1  1  1 -1 -1  1  1\n",
            "  1  1  1 -1  1  1 -1  1  1 -1  1 -1 -1  1  1  1  1  1  1  1  1  1  1 -1\n",
            "  1  1 -1  1 -1  1  1  1  1  1  1 -1 -1  1  1 -1  1  1 -1 -1  1 -1 -1  1\n",
            "  1 -1  1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1  1  1  1  1  1 -1  1  1 -1\n",
            " -1 -1  1  1 -1  1  1 -1 -1  1  1 -1 -1 -1 -1  1 -1  1  1  1  1  1 -1  1\n",
            "  1  1 -1 -1 -1  1 -1  1 -1 -1 -1 -1 -1  1  1  1  1  1  1 -1 -1  1  1  1\n",
            "  1 -1  1  1  1  1  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1 -1 -1  1 -1 -1 -1  1 -1  1  1  1  1  1 -1 -1  1  1  1  1  1\n",
            "  1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1 -1 -1  1 -1  1  1  1 -1 -1\n",
            "  1 -1 -1 -1 -1  1  1 -1 -1 -1  1 -1  1 -1  1  1  1 -1 -1  1  1  1 -1 -1\n",
            "  1 -1  1  1  1 -1  1  1  1 -1  1 -1  1 -1 -1  1  1  1  1  1 -1  1 -1 -1\n",
            "  1  1  1 -1 -1  1  1 -1 -1 -1  1 -1  1  1  1 -1  1 -1  1 -1 -1 -1 -1  1\n",
            "  1  1 -1  1  1 -1 -1  1 -1  1  1  1 -1  1 -1  1  1  1 -1  1 -1 -1  1  1\n",
            " -1  1 -1  1  1  1  1 -1  1  1  1 -1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
            "  1 -1 -1  1  1 -1  1  1  1  1 -1 -1  1 -1 -1  1  1  1  1  1 -1 -1  1  1\n",
            "  1  1  1 -1  1 -1  1 -1  1  1  1 -1  1  1  1  1 -1  1  1  1  1  1 -1  1\n",
            " -1  1 -1  1  1 -1 -1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1\n",
            "  1  1  1  1  1  1  1  1 -1 -1 -1  1  1 -1 -1 -1  1 -1 -1  1 -1  1  1 -1\n",
            "  1  1  1 -1  1  1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1  1  1  1  1  1  1\n",
            "  1  1  1 -1  1  1  1 -1  1 -1  1  1  1 -1 -1 -1  1  1  1  1  1  1 -1 -1\n",
            " -1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
            " -1  1  1 -1  1  1  1 -1 -1  1 -1  1 -1  1 -1  1  1 -1 -1 -1  1 -1  1 -1\n",
            "  1  1 -1  1  1 -1  1 -1  1  1 -1  1  1  1 -1 -1  1  1  1  1 -1 -1  1 -1\n",
            "  1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1  1  1 -1 -1  1  1 -1 -1\n",
            "  1 -1  1 -1 -1  1  1 -1  1  1  1  1 -1 -1 -1  1  1 -1  1  1 -1 -1  1  1\n",
            "  1  1 -1 -1  1 -1  1 -1 -1  1 -1 -1 -1  1  1 -1 -1  1  1  1 -1 -1 -1 -1\n",
            "  1 -1  1  1  1  1 -1  1 -1  1 -1  1  1  1  1  1 -1  1  1  1  1  1 -1 -1\n",
            "  1 -1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1 -1  1\n",
            " -1  1  1  1 -1  1 -1  1  1  1  1  1  1 -1  1 -1  1 -1 -1 -1  1  1  1  1\n",
            "  1  1  1  1 -1 -1  1 -1 -1  1  1  1 -1 -1 -1  1  1  1  1 -1 -1  1 -1 -1\n",
            "  1  1  1  1  1  1  1  1 -1 -1  1  1 -1  1  1  1  1  1 -1  1  1  1  1  1\n",
            "  1  1  1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1 -1  1  1 -1  1 -1\n",
            "  1  1  1  1 -1  1  1 -1 -1  1  1  1 -1  1  1 -1 -1  1 -1  1 -1  1  1 -1\n",
            " -1  1  1  1  1  1 -1 -1 -1  1  1  1  1  1  1  1 -1 -1  1 -1  1 -1 -1  1\n",
            "  1 -1 -1  1 -1  1  1 -1  1 -1  1  1  1  1  1  1  1  1 -1  1  1  1  1 -1\n",
            "  1 -1  1 -1  1  1  1 -1 -1  1  1  1 -1  1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id = pd.read_csv(\"/content/test _no_label.csv\", encoding=\"utf-8\", dtype={'ID': int})\n",
        "\n",
        "print(len(id))\n",
        "#print(len(predicted_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhIGEaCA8xyI",
        "outputId": "822c4a0e-57aa-40cf-b746-b6dd13ca6f02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "id = pd.read_csv(\"/content/test _no_label.csv\", encoding=\"utf-8\", dtype={'ID': int})\n",
        "\n",
        "# Assuming you have a column named 'ID' and 'Predicted_Label' in the DataFrame 'id'\n",
        "with open(\"test_with_label.csv\", mode=\"w\", newline=\"\") as csvfile:\n",
        "    fieldnames = [\"ID\", \"rating\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for index, row in id.iterrows():\n",
        "        writer.writerow({\"ID\": row[\"ID\"], \"rating\": predicted_classes[index]})\n"
      ],
      "metadata": {
        "id": "DuoqZ06ziMy2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# Calculate evaluation metrics\n",
        "y_eval=pd.read_csv(\"/content/test_with_label (1).csv\", encoding=\"utf-8\", dtype={'rating': int})\n",
        "accuracy = accuracy_score(y_eval[\"rating\"],predicted_classes)\n",
        "precision = precision_score(y_eval[\"rating\"],predicted_classes, average='weighted')\n",
        "recall = recall_score(y_eval[\"rating\"],predicted_classes, average='weighted')\n",
        "f1 = f1_score(y_eval[\"rating\"],predicted_classes, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh2HczAx2Fzm",
        "outputId": "771e74eb-4d28-4cea-fe27-4cbcc3cccb36"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.964\n",
            "Precision: 0.9639840210009701\n",
            "Recall: 0.964\n",
            "F1-score: 0.963886942564031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jhEF6jnyOXKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "tYhrm2-3YZ_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "#for lstm\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "#regulatization\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Plus 1 for the padding token\n",
        "print(vocab_size)\n",
        "embedding_dim = 500\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "\n",
        "#80\n",
        "# First LSTM layer\n",
        "\n",
        "model.add(LSTM(60, return_sequences=True))  # Return sequences for stacking\n",
        "model.add(Dropout(0.5))  # Regularization with Dropout\n",
        "\n",
        "# Second LSTM layer\n",
        "# Can also be a Bidirectional LSTM\n",
        "\n",
        "model.add(Bidirectional(LSTM(60)))  # Using a bidirectional LSTM layer\n",
        "model.add(Dropout(0.6))\n",
        "\n",
        "# Dense layers\n",
        "model.add(Dense(20, activation='relu',kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(10, activation='sigmoid',kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))\n",
        "model.add(Dropout(0.6))\n",
        "#model.add(Dense(5, activation='tanh',kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))\n",
        "# Output layer   solve overfitting\n",
        "model.add(Dense(3, activation='softmax'))  # Assuming 3 classes for classification'''\n",
        "\n",
        "#60%\n",
        "'''odel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(LSTM(20))  # Return sequences for stacking\n",
        "model.add(Dropout(0.5))  # Regularization with Dropout\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))  # Reduced dense layer complexity\n",
        "model.add(Dense(3, activation='softmax'))  # Output layer\n",
        "'''\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "# Compile the model\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "#solve overfitting 1-early stop\n",
        "'''early_stopping = EarlyStopping(\n",
        "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "    patience=20, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")'''\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_padded, y_train_encoded, epochs=10, batch_size=32,validation_split=0.2)\n",
        "\n",
        "\n",
        "Predict_sentiments(model,X_test_padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cx4t-4V5YScm",
        "outputId": "d4886f6d-db14-404e-ee1d-5837e4a19231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12436\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 569, 500)          6218000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 569, 60)           134640    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 569, 60)           0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 120)               58080     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                2420      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                210       \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 10)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6413383 (24.47 MB)\n",
            "Trainable params: 6413383 (24.47 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "795/795 [==============================] - 751s 939ms/step - loss: 0.8458 - accuracy: 0.5906 - val_loss: 0.7252 - val_accuracy: 0.7290\n",
            "Epoch 2/10\n",
            "795/795 [==============================] - 787s 990ms/step - loss: 0.6945 - accuracy: 0.7379 - val_loss: 0.5940 - val_accuracy: 0.8043\n",
            "Epoch 3/10\n",
            " 27/795 [>.............................] - ETA: 12:41 - loss: 0.6174 - accuracy: 0.7951"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-10-de58f67c727e>\", line 66, in <cell line: 66>\n",
            "    history = model.fit(X_padded, y_train_encoded, epochs=10, batch_size=32,validation_split=0.2)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 832, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 868, in _call\n",
            "    return tracing_compilation.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1323, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
            "    flat_outputs = self.call_flat(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
            "    outputs = self._bound_context.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\", line 1486, in call_function\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 431, in _joinrealpath\n",
            "    st = os.lstat(newpath)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-de58f67c727e>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    }
  ]
}